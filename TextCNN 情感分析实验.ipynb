{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入 Python 库&模块并配置运行信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import (\n",
    "    ModelCheckpoint,\n",
    "    CheckpointConfig,\n",
    "    LossMonitor,\n",
    "    TimeMonitor,\n",
    ")\n",
    "from mindspore.ops import operations as ops\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict(\n",
    "    {\n",
    "        \"name\": \"moive review\",\n",
    "        \"pre_trained\": False,\n",
    "        \"num_classes\": 2,\n",
    "        \"batch_size\": 64,\n",
    "        \"epoch_size\": 4,\n",
    "        \"weight_decay\": 3e-5,\n",
    "        \"data_path\": \"./data/\",\n",
    "        \"device_target\": \"CPU\",\n",
    "        \"device_id\": 0,\n",
    "        \"keep_checkpoint_max\": 1,\n",
    "        \"checkpoint_path\": \"./ckpt/train_textcnn-4_149.ckpt\",\n",
    "        \"word_len\": 51,\n",
    "        \"vec_length\": 40,\n",
    "    }\n",
    ")\n",
    "\n",
    "context.set_context(\n",
    "    mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据读取和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/rt-polarity.neg\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"Negative reviews:\")\n",
    "    for i in range(5):\n",
    "        print(\"[{0}]:{1}\".format(i, f.readline()))\n",
    "\n",
    "with open(\"./data/rt-polarity.pos\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"Positive reviews:\")\n",
    "    for i in range(5):\n",
    "        print(\"[{0}]:{1}\".format(i, f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据处理函数代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据处理函数代码\n",
    "class Generator:\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list = input_list\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (\n",
    "            np.array(self.input_list[item][0], dtype=np.int32),\n",
    "            np.array(self.input_list[item][1], dtype=np.int32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    \"\"\"\n",
    "    影评数据集\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\"neg\": 0, \"pos\": 1}\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "\n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            raise ValueError(\"please check the root_dir!\")\n",
    "\n",
    "        # 在数据目录下寻找文件\n",
    "        for root, _, filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root, each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg和.pos\n",
    "        if len(self.files) != 2:\n",
    "            raise ValueError(\n",
    "                \"There are {} files in the root_dir\".format(len(self.files))\n",
    "            )\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            self.read_data(filename)\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_data(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "        with open(filePath, \"r\") as f:\n",
    "            for sentence in f.readlines():\n",
    "                sentence = (\n",
    "                    sentence.replace(\"\\n\", \"\")\n",
    "                    .replace('\"', \"\")\n",
    "                    .replace(\"'\", \"\")\n",
    "                    .replace(\".\", \"\")\n",
    "                    .replace(\",\", \"\")\n",
    "                    .replace(\"[\", \"\")\n",
    "                    .replace(\"]\", \"\")\n",
    "                    .replace(\"(\", \"\")\n",
    "                    .replace(\")\", \"\")\n",
    "                    .replace(\":\", \"\")\n",
    "                    .replace(\"--\", \"\")\n",
    "                    .replace(\"-\", \"\")\n",
    "                    .replace(\"\\\\\", \"\")\n",
    "                    .replace(\"0\", \"\")\n",
    "                    .replace(\"1\", \"\")\n",
    "                    .replace(\"2\", \"\")\n",
    "                    .replace(\"3\", \"\")\n",
    "                    .replace(\"4\", \"\")\n",
    "                    .replace(\"5\", \"\")\n",
    "                    .replace(\"6\", \"\")\n",
    "                    .replace(\"7\", \"\")\n",
    "                    .replace(\"8\", \"\")\n",
    "                    .replace(\"9\", \"\")\n",
    "                    .replace(\"`\", \"\")\n",
    "                    .replace(\"=\", \"\")\n",
    "                    .replace(\"$\", \"\")\n",
    "                    .replace(\"/\", \"\")\n",
    "                    .replace(\"*\", \"\")\n",
    "                    .replace(\";\", \"\")\n",
    "                    .replace(\"<b>\", \"\")\n",
    "                    .replace(\"%\", \"\")\n",
    "                )\n",
    "                # 为什么不用正则？\n",
    "                # import re\n",
    "                # sentence = re.sub(r'[\\n\"\\'.,\\[\\]\\(\\):\\\\\\-0-9`=/$*;%<b>%]', '', sentence)\n",
    "                sentence = sentence.split(\" \")\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = max(self.maxlen, len(sentence))\n",
    "                    self.minlen = min(self.minlen, len(sentence))\n",
    "                    if \"pos\" in filePath:\n",
    "                        self.Pos.append([sentence, self.feelMap[\"pos\"]])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence, self.feelMap[\"neg\"]])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        for SentenceLabel in self.Pos + self.Neg:\n",
    "            vector = [0] * maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        trunk_pos_size = math.ceil((1 - split) * len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1 - split) * len(self.Neg))\n",
    "        trunk_num = int(1 / (1 - split))\n",
    "        pos_temp = list()\n",
    "        neg_temp = list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(\n",
    "                self.Pos[index * trunk_pos_size : (index + 1) * trunk_pos_size]\n",
    "            )\n",
    "            neg_temp.append(\n",
    "                self.Neg[index * trunk_neg_size : (index + 1) * trunk_neg_size]\n",
    "            )\n",
    "        self.test = pos_temp.pop(2) + neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp + neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec!\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.train),\n",
    "            column_names=[\"data\", \"label\"],\n",
    "            shuffle=False,\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        dataset = dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.test),\n",
    "            column_names=[\"data\", \"label\"],\n",
    "            shuffle=False,\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(\n",
    "    batch_size=cfg.batch_size, epoch_size=cfg.epoch_size\n",
    ")\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示数据处理结果代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = instance.get_dict_len()\n",
    "print(\"vocab_size: \", vocab_size)\n",
    "item = dataset.create_dict_iterator()\n",
    "for i, data in enumerate(item):\n",
    "    if i < 1:\n",
    "        print(data)\n",
    "        print(data[\"data\"][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [\n",
    "    1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1)\n",
    "    for _ in range(batch_num)\n",
    "    for i in range(math.floor(cfg.epoch_size / 5))\n",
    "]\n",
    "shrink = [\n",
    "    1e-3 / (16 * (i + 1))\n",
    "    for _ in range(batch_num)\n",
    "    for i in range(math.floor(cfg.epoch_size * 3 / 5))\n",
    "]\n",
    "normal_run = [\n",
    "    1e-3\n",
    "    for _ in range(batch_num)\n",
    "    for i in range(\n",
    "        cfg.epoch_size\n",
    "        - math.floor(cfg.epoch_size / 5)\n",
    "        - math.floor(cfg.epoch_size * 2 / 5)\n",
    "    )\n",
    "]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextCNN 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=96,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=1,\n",
    "        pad_mode=\"pad\",\n",
    "        weight_init=weight,\n",
    "        has_bias=True,\n",
    "    )\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_len, self.vec_length, embedding_table=\"normal\"\n",
    "        )\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = make_conv_layer(kernel_size=3)\n",
    "        self.layer2 = make_conv_layer(kernel_size=4)\n",
    "        self.layer3 = make_conv_layer(kernel_size=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96 * 3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "\n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height, self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len - kernel_height + 1, 1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = TextCNN(\n",
    "    vocab_len=instance.get_dict_len(),\n",
    "    word_len=cfg.word_len,\n",
    "    num_classes=cfg.num_classes,\n",
    "    vec_length=cfg.vec_length,\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练的相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器、损失函数、保存检查点、时间监视器等设置\n",
    "opt = nn.Adam(\n",
    "    filter(lambda x: x.requires_grad, net.get_parameters()),\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={\"acc\": Accuracy()})\n",
    "config_ck = CheckpointConfig(\n",
    "    save_checkpoint_steps=int(cfg.epoch_size * batch_num / 2),\n",
    "    keep_checkpoint_max=cfg.keep_checkpoint_max,\n",
    ")\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(\n",
    "    prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck\n",
    ")\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = (\n",
    "        sentence.replace(\"\\n\", \"\")\n",
    "        .replace('\"', \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"[\", \"\")\n",
    "        .replace(\"]\", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\":\", \"\")\n",
    "        .replace(\"--\", \"\")\n",
    "        .replace(\"-\", \"\")\n",
    "        .replace(\"\\\\\", \"\")\n",
    "        .replace(\"0\", \"\")\n",
    "        .replace(\"1\", \"\")\n",
    "        .replace(\"2\", \"\")\n",
    "        .replace(\"3\", \"\")\n",
    "        .replace(\"4\", \"\")\n",
    "        .replace(\"5\", \"\")\n",
    "        .replace(\"6\", \"\")\n",
    "        .replace(\"7\", \"\")\n",
    "        .replace(\"8\", \"\")\n",
    "        .replace(\"9\", \"\")\n",
    "        .replace(\"`\", \"\")\n",
    "        .replace(\"=\", \"\")\n",
    "        .replace(\"$\", \"\")\n",
    "        .replace(\"/\", \"\")\n",
    "        .replace(\"*\", \"\")\n",
    "        .replace(\";\", \"\")\n",
    "        .replace(\"<b>\", \"\")\n",
    "        .replace(\"%\", \"\")\n",
    "        .replace(\"  \", \" \")\n",
    "    )\n",
    "    sentence = sentence.split(\" \")\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0] * maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(\"word {} not in vocab\".format(word))\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def inference(revire_en):\n",
    "    revire_en = preprocess(revire_en)\n",
    "    input_en = Tensor(np.array(revire_en), np.int32)\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revire_en = \"the movie is so boring\"\n",
    "inference(revire_en)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
